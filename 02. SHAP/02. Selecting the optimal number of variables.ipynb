{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79810ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c16fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Reproducibility\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ddd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Paths (edit as needed)\n",
    "\n",
    "model_path_in  = r\"C:/Users/hangang/Desktop/best_xgb_model_full.pkl\"\n",
    "data_path      = r\"C:/Users/hangang/Desktop/01_data_full.csv\"\n",
    "model_path_out = r\"C:/Users/hangang/Desktop/best_XGB_model_topk.pkl\"\n",
    "log_path_out   = r\"C:/Users/hangang/Desktop/topk_search_log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load saved artifact (expects dict with model, scaler, features)\n",
    "# =========================\n",
    "with open(model_path_in, \"rb\") as f:\n",
    "    obj = pickle.load(f)\n",
    "\n",
    "if not (isinstance(obj, dict) and \"model\" in obj):\n",
    "    raise ValueError(\"Saved artifact must be a dict with keys {'model','scaler','features'}.\")\n",
    "\n",
    "base_model    = obj[\"model\"]        # only used to keep consistency; we will retrain new models per K\n",
    "saved_scaler  = obj.get(\"scaler\", None)\n",
    "saved_features= obj.get(\"features\", None)\n",
    "\n",
    "if saved_scaler is None:\n",
    "    raise ValueError(\"No scaler found in the saved artifact.\")\n",
    "\n",
    "# scaler class to refit per loop (avoid leakage)\n",
    "scaler_cls = saved_scaler.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dea2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load data and split\n",
    "# =========================\n",
    "df = pd.read_csv(data_path, encoding=\"utf-8\")\n",
    "if \"Chl-a\" not in df.columns:\n",
    "    raise KeyError(\"'Chl-a' column not found in the CSV.\")\n",
    "\n",
    "X_all = df.drop(\"Chl-a\", axis=1)\n",
    "y_all = df[\"Chl-a\"]\n",
    "\n",
    "# align to saved feature order if present\n",
    "if saved_features is not None:\n",
    "    missing = [c for c in saved_features if c not in X_all.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing features in CSV: {missing}\")\n",
    "    X_all = X_all[saved_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Helper: metrics and hyperopt space (same ranges as before)\n",
    "# =========================\n",
    "def metrics(y_true, y_pred):\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    return r2, rmse, mae\n",
    "\n",
    "space = {\n",
    "    'n_estimators':     hp.quniform('n_estimators', 10, 80, 1),\n",
    "    'max_depth':        hp.quniform('max_depth', 1, 7, 1),\n",
    "    'learning_rate':    hp.uniform('learning_rate', 0.01, 0.08),\n",
    "    'gamma':            hp.uniform('gamma', 0.0, 6.0),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 20, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.15, 1.0),\n",
    "    'alpha':            hp.uniform('alpha', 0.0, 6.0),\n",
    "    'lambda':           hp.uniform('lambda', 0.0, 6.0),\n",
    "}\n",
    "\n",
    "def build_xgb(params):\n",
    "    return xgb.XGBRegressor(\n",
    "        n_estimators     = int(params['n_estimators']),\n",
    "        max_depth        = int(params['max_depth']),\n",
    "        learning_rate    = float(params['learning_rate']),\n",
    "        gamma            = float(params['gamma']),\n",
    "        min_child_weight = int(params['min_child_weight']),\n",
    "        colsample_bytree = float(params['colsample_bytree']),\n",
    "        reg_alpha        = float(params['alpha']),\n",
    "        reg_lambda       = float(params['lambda']),\n",
    "        random_state     = random_seed,\n",
    "        n_jobs           = -1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) SHAP ranking on TRAIN set (scaled with saved scaler class)\n",
    "# =========================\n",
    "scaler_for_rank = scaler_cls()\n",
    "Xtr_scaled_for_rank = scaler_for_rank.fit_transform(X_train)\n",
    "\n",
    "explainer = shap.TreeExplainer(base_model)\n",
    "# If base_model was trained on scaled data, ranking should also use scaled features\n",
    "shap_vals_train = explainer.shap_values(Xtr_scaled_for_rank, check_additivity=False)\n",
    "mean_abs = np.mean(np.abs(shap_vals_train), axis=0)\n",
    "rank_idx = np.argsort(-mean_abs)  # descending\n",
    "ranked_features = X_train.columns.to_numpy()[rank_idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa75eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Iterate K = 5..len(features), train/eval, pick best by R2 on TEST\n",
    "# =========================\n",
    "results = []\n",
    "best_record = None  # (k, r2_test, rmse_test, mae_test, params, model, fitted_scaler, features_k)\n",
    "\n",
    "for k in range(5, len(ranked_features) + 1):\n",
    "    feats_k = ranked_features[:k]\n",
    "\n",
    "    # fit scaler on TRAIN (subset columns) to avoid leakage\n",
    "    scaler_k = scaler_cls()\n",
    "    Xtr_k = scaler_k.fit_transform(X_train[feats_k])\n",
    "    Xte_k = scaler_k.transform(X_test[feats_k])\n",
    "\n",
    "    # hyperopt objective (3-fold CV on TRAIN)\n",
    "    def objective(p):\n",
    "        model_k = build_xgb(p)\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=random_seed)\n",
    "        cv_scores = cross_val_score(model_k, Xtr_k, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "        mean_mse = -float(np.mean(cv_scores))\n",
    "        return {'loss': mean_mse, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=200, trials=trials, rstate=np.random.default_rng(random_seed))\n",
    "    best_params = space_eval(space, best)\n",
    "\n",
    "    # train on full TRAIN with best params\n",
    "    model_k = build_xgb(best_params)\n",
    "    model_k.fit(Xtr_k, y_train)\n",
    "\n",
    "    # evaluate on TEST\n",
    "    yhat_te = model_k.predict(Xte_k)\n",
    "    r2_te, rmse_te, mae_te = metrics(y_test, yhat_te)\n",
    "\n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'r2_test': r2_te,\n",
    "        'rmse_test': rmse_te,\n",
    "        'mae_test': mae_te,\n",
    "        'best_params': best_params\n",
    "    })\n",
    "\n",
    "    # keep best by R2, tie-breaker by lower RMSE\n",
    "    if (best_record is None) or (r2_te > best_record[1]) or (np.isclose(r2_te, best_record[1]) and rmse_te < best_record[2]):\n",
    "        best_record = (k, r2_te, rmse_te, mae_te, best_params, model_k, scaler_k, feats_k)\n",
    "\n",
    "# save search log\n",
    "pd.DataFrame(results).to_csv(log_path_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save best model (with scaler & selected features)\n",
    "# =========================\n",
    "best_k, best_r2, best_rmse, best_mae, best_params, best_model, best_scaler, best_feats = best_record\n",
    "\n",
    "with open(model_path_out, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        'model': best_model,\n",
    "        'scaler': best_scaler,\n",
    "        'features': best_feats,\n",
    "        'random_seed': random_seed,\n",
    "        'best_params': best_params,\n",
    "        'ranked_features_all': ranked_features,\n",
    "        'best_k': best_k,\n",
    "        'cv_space': 'XGB ranges: n_estimators[10,80], max_depth[1,7], lr[0.01,0.08], gamma[0,6], min_child_weight[1,20], colsample_bytree[0.15,1], alpha[0,6], lambda[0,6]'\n",
    "    }, f)\n",
    "\n",
    "print(f\"[Best K] {best_k}\")\n",
    "print(f\"[Test R2/RMSE/MAE] {best_r2:.4f} / {best_rmse:.4f} / {best_mae:.4f}\")\n",
    "print(f\"[Saved] {model_path_out}\")\n",
    "print(f\"[Search log] {log_path_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965d98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f88da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c45e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140028bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
